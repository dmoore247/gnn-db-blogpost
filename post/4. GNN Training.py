# Databricks notebook source
# MAGIC %md 
# MAGIC # 4. Implementation: GNN Training

# COMMAND ----------

# MAGIC %md-sandbox ## 4.1 Machine Learning
# MAGIC <div>
# MAGIC   <img src="https://github.com/grandintegrator/gnn-db-blogpost/blob/main/media/ml-workflow.png?raw=True" alt="graph-training" width="840px", />
# MAGIC </div>

# COMMAND ----------

# MAGIC %run ./setup

# COMMAND ----------

import mlflow

# COMMAND ----------

# DBTITLE 1,Create notebook widgets for database name and dataset paths
dbutils.widgets.text(name="catalog_name", defaultValue="gnn_blog", label="Catalog Name")
dbutils.widgets.text(name="database_name", defaultValue="gnn_blog_db", label="Database Name")
dbutils.widgets.text(name="experiment_name", defaultValue="/Repos/douglas.moore@databricks.com/gnn-db-blogpost-dev/post/4. GNN Training", label="experiment_name")

# COMMAND ----------

# DBTITLE 1,Unzip data and choose a database for analysis
catalog_name = dbutils.widgets.get("catalog_name")
database_name = dbutils.widgets.get("database_name")
experiment_name = dbutils.widgets.get("experiment_name")

model_name = f"supply_gnn_model_{catalog_name}_{database_name}"

mlflow.set_experiment(experiment_name)
spark.sql(f"use {catalog_name}.{database_name};")

# COMMAND ----------

# MAGIC %md-sandbox
# MAGIC
# MAGIC ## 4.2 Training our Graph Neural Network
# MAGIC
# MAGIC <div style="float:right">
# MAGIC   <img src="https://github.com/grandintegrator/gnn-db-blogpost/blob/main/media/training_GNNs.png?raw=True" alt="graph-training" width="840px", />
# MAGIC </div>
# MAGIC
# MAGIC We will train our GNN in the mini-batch setting since this format scales well as the number of nodes in the graph grow, this method of training is also referred to as stochastic training of GNNs. We will leverage a graph data loader class from dgl to facilitate the mini-batch training. Graph data loaders are task dependent. For link prediction, the GNN is trained on **edge batches**. If the task were node focused (node classification/regression) then the graph would be partitioned based on nodes and we would use a node data loader. 
# MAGIC
# MAGIC Before creating the edge bunches, we partition the graph into a training, validation, and testing graph. For each of the graph splits we negatively sample negative edges (edges that do not exist within the current graph). The negative sampling scheme can get quite complex but we'll maintain a simple negative sampler for this blog. During training batches, a positive graph (actual set of connections) and a negative graph (from our negatively sampled edges) will be used to train the GNN. The GNN loss will incentivise the embeddings to score the likelihood of the negative edges as lower than real edges.

# COMMAND ----------

silver_relation_table = spark.read.format('delta').table('silver_relation_data')

# COMMAND ----------

# DBTITLE 1,We begin by making a DGL (Deep Graph Library) Graph object from our Silver table
silver_relation_table = spark.read.format('delta').table('silver_relation_data')

def make_dgl_graph(relation_table) -> dgl.DGLGraph:
    """
    Make a Graph object within DGL using src_id and dst_id from table encoding graph
    Nodes: Companies with unique src_id or dst_id
    Edges: Company A (with src_id) --> buys_from --> Company B (with dst_id)
    """
    triplets = [(x.src_id, x.dst_id) for x in relation_table.select("src_id", "dst_id").collect()]
    return dgl.graph(triplets)

supplier_graph = make_dgl_graph(relation_table=silver_relation_table)

# COMMAND ----------

# DBTITLE 1,We can inspect our graph to see the number of edges and nodes
supplier_graph

# COMMAND ----------

# DBTITLE 1,Split out graph to create graph cross-validation sets (training graph, validation graph, and testing graph)
def make_graph_partitions(graph: dgl.DGLGraph,
                          params: Dict[str, Any]) -> Dict[str, dgl.DGLGraph]:
    """
    Function creates graph partitions consisting of:
    1. training graph: initial tuning of parameters
    2. valid graph: final tuning
    3. testing graph: for reporting final accuracy statistics
    """
    src_ids, dst_ids = graph.edges()

    test_size = int(params['test_p'] * len(src_ids))
    valid_size = int(params['valid_p'] * len(dst_ids))
    train_size = len(src_ids) - valid_size - test_size

    # Source and destinations for training
    src_train, dst_train = src_ids[0:train_size], dst_ids[0:train_size]

    # Source and destinations for validation
    src_valid = src_ids[train_size: valid_size + train_size]
    dst_valid = dst_ids[train_size: valid_size + train_size]

    # Source and destinations for testing
    src_test = src_ids[valid_size + train_size:]
    dst_test = dst_ids[valid_size + train_size:]

    return {'training': dgl.graph((src_train, dst_train)),
            'validation': dgl.graph((src_valid, dst_valid)),
            'testing': dgl.graph((src_test, dst_test))}

# COMMAND ----------

# DBTITLE 1,Create Data Loaders in the form of edges (positive and negative) since we are performing Link Prediction
def get_edge_dataloaders(graph_partitions: Dict[str, dgl.DGLGraph], 
                         params: Dict[str, Any]) -> (Dict[str, dgl.dataloading.EdgeDataLoader],
                                                     Dict[str, dgl.DGLGraph],
                                                     dgl.DGLGraph):
    """
    Returns an edge loader for link prediction for each partition of the graph 
    with sampling function and negative sampling function as well as the graph partitions.
    """
    data_loaders = {}
    sampler = dgl.dataloading.MultiLayerNeighborSampler([4, 4])
    
    # Draws samples of non-existent edges from the uniform distribution
    negative_sampler = (
      dgl.dataloading.negative_sampler.Uniform(params['num_negative_samples'])
    )

    for split in graph_partitions.keys():
        # Create a feature vector for every node [1 x num_node_features]
        graph_partitions[split].ndata['feature'] = (
            torch.randn(graph_partitions[split].num_nodes(),
                        params['num_node_features'])
        )

        # Create the data loader based on the sampler and negative sampler
        data_loaders[split] = dgl.dataloading.EdgeDataLoader(
            graph_partitions[split],
            graph_partitions[split].edges(form='eid'),
            sampler,
            negative_sampler=negative_sampler,
            batch_size=params['batch_size'],
            shuffle=True,
            drop_last=False,
            num_workers=params['num_workers'])

    return data_loaders

# COMMAND ----------

# MAGIC %md-sandbox
# MAGIC ### 4.2.1 Defining the Graph Neural Network model
# MAGIC
# MAGIC <div style="float:right">
# MAGIC   <img src="https://github.com/grandintegrator/gnn-db-blogpost/blob/main/media/architecture.png?raw=True" alt="graph-training" width="700px", />
# MAGIC </div>
# MAGIC
# MAGIC Our GNN model will consist of two GraphSAGE layers to generate node embeddings and will be trained using the edge data loaders we have defined above. The embeddings are then fed into a seperate (simple) neural network that will take as inputs the embeddings for source and destination nodes and provide a prediction for the likelihood of a link (binary classification). More formally, the neural network acts as \\( f: (\mathbf{h}_u, \mathbf{h}_v )\rightarrow z{_u}{_v} \\). All of the network weights are trained using a single loss function, either a binary cross entropy loss or a margin loss. During training and validation we collect pseudo-accuracy metrics like the loss and the ROC-AUC and use mlflow to track the metrics during training.

# COMMAND ----------

# DBTITLE 1,Define a simple Neural Network to classify learned embedding vectors for nodes
class MLPPredictor(nn.Module):
    def __init__(self, in_features):
        super().__init__()
        #                      source and destination nodes (each will have their own features)
        #          num_node_features     |
        #                    |           |
        self.W = nn.Linear(in_features * 2, 1)
        #                                   |
        #                                   |
        #                                  binary classification!
    def apply_edges(self, edges):
        h_u = edges.src['h']
        h_v = edges.dst['h']
        score = self.W(cat([h_u, h_v], 1))
        return {'score': torch.sigmoid(score)}

    def forward(self, graph, h):
        with graph.local_scope():
            graph.ndata['h'] = h
            graph.apply_edges(self.apply_edges)
            return graph.edata['score']

# COMMAND ----------

# DBTITLE 1,Layers: Defining the GraphSAGE layer for inductive learning
class GraphSAGE(nn.Module):
    def __init__(self, in_feats, hid_feats, out_feats, aggregator_type):
        super().__init__()
        self.in_feats = in_feats
        self.hid_feats = hid_feats
        self.out_feats = out_feats
        self.n_layers = 2

        self.conv1 = SAGEConv(in_feats=in_feats, out_feats=hid_feats,
                              aggregator_type=aggregator_type)
        self.conv2 = SAGEConv(in_feats=hid_feats, out_feats=out_feats,
                              aggregator_type=aggregator_type)

    def forward(self, blocks, inputs):
        # inputs are features of nodes
        h = F.relu(self.conv1(blocks[0], inputs))
        h = self.conv2(blocks[1], h)
        return h

    def inference(self, g, x, batch_size, device='cpu'):
        """
        Based on the graph and feature set, return the embeddings from
        the final layer of the GraphSAGE model.
        """
        # Compute representations layer by layer
        for l, layer in enumerate([self.conv1, self.conv2]):
            y = torch.zeros(g.number_of_nodes(), self.hid_feats
                            if l != self.n_layers - 1
                            else self.out_feats)
            sampler = dgl.dataloading.MultiLayerFullNeighborSampler(1)
            dataloader = dgl.dataloading.NodeDataLoader(
                g, torch.arange(g.number_of_nodes()), sampler,
                batch_size=batch_size,
                shuffle=True,
                drop_last=False)

            # Within a layer, iterate over nodes in batches
            for input_nodes, output_nodes, blocks in dataloader:
                block = blocks[0]
                h = x[input_nodes].to(device)
                h_dst = h[:block.number_of_dst_nodes()]
                h = F.relu(layer(block, (h, h_dst)))
                y[output_nodes] = h.cpu()

            x = y
        return y

# COMMAND ----------

# DBTITLE 1,Putting it all together with GraphSAGE to generate embeddings and MLPPredictor to classify links
class Model(nn.Module):
    def __init__(self, in_features, hidden_features, out_features, num_classes,
                 aggregator_type):
        super().__init__()
        # Define the graph convolution flavour --> GraphSAGE
        self.gcn = GraphSAGE(in_features, hidden_features, out_features, aggregator_type)
        self.pred = MLPPredictor(out_features)

    def forward(self, positive_graph, negative_graph, blocks, x):
        x = self.gcn(blocks, x)
        pos_score = self.pred(positive_graph, x)
        neg_score = self.pred(negative_graph, x)
        return pos_score, neg_score
    
    def get_embeddings(self, g, x, batch_size, device, provide_prediction=False):
      if provide_prediction:
        return self.pred(g, x)
      else:
        return self.gcn.inference(g, x, batch_size, device)

# COMMAND ----------

# DBTITLE 1,We can now inspect our overall GNN + NN architecture
# Parameters are defined in setup, they are basic and are later tuned using HyperOpt
graph_model = Model(in_features=params['num_node_features'],
                    hidden_features=params['num_hidden_graph_layers'],
                    out_features=params['num_node_features'],
                    num_classes=params['num_classes'],
                    aggregator_type=params['aggregator_type'])

graph_model

# COMMAND ----------

# MAGIC %md-sandbox
# MAGIC ## 4.2.2 Training the Graph Neural Network
# MAGIC The defined architecture's forward passes have been defined but the weights of the layers need to be trained. We define a training and validation scheme below to optimise the network weights but also use HyperOpt to search the wider design space. This includes searching the space parameters like the number of negative samples per positive sample, and the dimensionality of the number of node features for the GNN to name a few.

# COMMAND ----------

# DBTITLE 1,We define a trainer class for training the Model defined above
class Trainer(object):
    def __init__(self, params: Dict[str, Any], model: torch.nn.Module,
                 train_data_loader: dgl.dataloading.EdgeDataLoader,
                 training_graph: dgl.DGLGraph):
        self.params = params
        self.model = model
        self.training_graph = training_graph
        self.train_data_loader = train_data_loader
        self.opt = None
        self.sigmoid = nn.Sigmoid()

    def make_optimiser(self):
        # Experiment with 2 seperate optimisers (set by the params)
        if self.params['optimiser'] == 'SGD':
            self.opt = optim.SGD(self.model.parameters(), lr=self.params['lr'],
                                 momentum=self.params['momentum'],
                                 weight_decay=self.params['l2_regularisation'])
        elif self.params['optimiser'] == 'Adam':
            self.opt = optim.Adam(self.model.parameters(),
                                  lr=self.params['lr'],
                                  weight_decay=self.params['l2_regularisation'])

    def compute_loss(self, pos_score, neg_score):
        pos_score_edge = self.sigmoid(pos_score)
        neg_score_edge = self.sigmoid(neg_score)
        n = pos_score.shape[0]
        if self.params['loss'] == 'margin':
            margin_loss = \
                (neg_score_edge.view(n, -1) - pos_score.view(n, -1) + 1) \
                    .clamp(min=0).mean()
            return margin_loss
        elif self.params['loss'] == 'binary_cross_entropy':
            scores = cat([pos_score, neg_score])
            labels = cat([ones(pos_score.shape[0]),
                          zeros(neg_score.shape[0])])
            scores = scores.view(len(scores), -1).mean(dim=1)
            binary_cross_entropy = F.binary_cross_entropy_with_logits(scores, labels)
            return binary_cross_entropy

    def train_epochs(self):
        self.make_optimiser()
        self.model.train()
        
        with tqdm.tqdm(self.train_data_loader) as tq:
            for step, (input_nodes, positive_graph, negative_graph,
                       blocks) in enumerate(tq):
                
                if self.params['device'] == 'cuda':
                    blocks = [b.to(torch.device('cuda')) for b in blocks]
                    positive_graph = positive_graph.to(torch.device('cuda'))
                    negative_graph = negative_graph.to(torch.device('cuda'))
                
                # The blocks will contain the feature information of the nodes
                input_features = blocks[0].srcdata['feature']
                
                # The pos and neg scores are just binary outputs 
                pos_score, neg_score = self.model(positive_graph=positive_graph,
                                                  negative_graph=negative_graph,
                                                  blocks=blocks, x=input_features)
                loss = self.compute_loss(pos_score, neg_score)

                # 🔙  Back Prop :)
                self.opt.zero_grad()
                loss.backward()
                self.opt.step()

                # Logging
                results = compute_auc_ap(pos_score, neg_score)
                mlflow.log_metric("Epoch Loss", float(loss.item()), step=step)
                mlflow.log_metric("Model Training AUC", results['AUC'], step=step)
                tq.set_postfix({'loss': '%.03f' % loss.item()}, refresh=False)

                # Break if number of epochs has been satisfied
                if step == self.params['num_epochs']:
                    break

        return {'model': self.model, 'final_training_auc': results['AUC'], 'loss': loss.item()}

# COMMAND ----------

# DBTITLE 1,Define an evaluator which samples subgraphs in the validation graph and returns average ROC across the sampled subgraphs
def evaluate(trained_model: torch.nn.Module,
             validation_data_loader: dgl.dataloading.EdgeDataLoader) -> (List, List):
  trained_model.eval()
  roc_auc_sugraphs = []
  ap_sugraphs = []
  for input_nodes, positive_graph, negative_graph, blocks in validation_data_loader:
    with torch.no_grad():
        input_features = blocks[0].srcdata['feature']
        
        # 🔜 Forward pass through the network.
        pos_score, neg_score = \
            trained_model(positive_graph=positive_graph,
                          negative_graph=negative_graph,
                          blocks=blocks,
                          x=input_features)
        auc_pr_dicts = compute_auc_ap(pos_score, neg_score)
        roc_auc_sugraphs.append(auc_pr_dicts['AUC'])
        ap_sugraphs.append(auc_pr_dicts['AP'])
  return roc_auc_sugraphs, ap_sugraphs

# COMMAND ----------

# MAGIC %md-sandbox
# MAGIC ## 4.2.3 Searching the design space using HyperOpt

# COMMAND ----------

# DBTITLE 1,Now that we have defined our models, we define a search space for our network and graphs samplers and features
params_hyperopt = {'optimiser': hp.choice('optimiser', ["Adam", "SGD"]),
                   'loss': hp.choice('loss', ['binary_cross_entropy', 'margin']),
                   'num_node_features': hp.choice('num_node_features', [2, 10, 20, 50, 100]),
                   'num_hidden_graph_layers': hp.choice('num_hidden_graph_layers', [5, 15, 100]),
                   'batch_size': hp.choice('batch_size', [16, 32, 64, 128]),
                   'num_negative_samples': hp.choice('num_negative_samples', [3, 10, 50]),
                   'num_epochs': hp.choice('num_epochs', [1000, 2000, 8000]),
                   'l2_regularisation': hp.choice('l2_regularisation', [5e-4, 5e-3, 5e-2]),
                   'momentum': hp.choice('momentum', [5e-4, 5e-3, 5e-2]),
                   'lr': hp.choice('lr', [1e-3, 1e-5, 1e-2]),
                   'aggregator_type': hp.choice('aggregator_type', ['mean', 'pool'])}
static_params = {'device': 'cpu',
                 'test_p': 0.1,
                 'valid_p': 0.2,
                 'num_workers': 0,
                 'num_classes': 2}
params_hyperopt = {**params_hyperopt, **static_params}

# COMMAND ----------

# DBTITLE 1,We will use HyperOpt to search the GNN and graph-sampling parameter space
# Let's create a static training, validation, and testing set of graphs
graph_partitions = make_graph_partitions(graph=supplier_graph, params=params)

def train_and_evaluate_gnn(params_hyperopt: Dict[str, Any]) -> Dict[str, Any]:
  # New set of data loaders given the parameter set
  graph_model = Model(in_features=params_hyperopt['num_node_features'],
                      hidden_features=params_hyperopt['num_hidden_graph_layers'],
                      out_features=params_hyperopt['num_node_features'],
                      num_classes=params_hyperopt['num_classes'],
                      aggregator_type=params_hyperopt['aggregator_type'])

  data_loaders = get_edge_dataloaders(graph_partitions=graph_partitions,
                                      params=params_hyperopt)

  trainer = Trainer(params=params_hyperopt,
                    model=graph_model,
                    train_data_loader=data_loaders['training'],
                    training_graph=graph_partitions['training'])
  training_results = trainer.train_epochs()

  # The trained model based on the hyperparams is now fed into the validation
  roc_auc_sugraphs, _ = evaluate(trained_model=training_results['model'], 
                                 validation_data_loader=data_loaders['validation'])

  # We want to optimise for the highest auc accross the validation subgraphs
  loss = -1*sum(roc_auc_sugraphs)/len(roc_auc_sugraphs)

  return {'loss': loss, 'status': STATUS_OK,
          'param_hyperopt': params_hyperopt}

# COMMAND ----------

argmin = fmin(fn=train_and_evaluate_gnn,
            space=params_hyperopt,
            algo=tpe.suggest,
            max_evals=20,
            trials=SparkTrials(parallelism=8))

# COMMAND ----------

ran_hyperopt = False
if ran_hyperopt:
  best_parameters = hyperopt.space_eval(params_hyperopt, argmin)
else: 
  best_parameters = {'aggregator_type': 'pool',
   'batch_size': 32,
   'device': 'cpu',
   'l2_regularisation': 0.005,
   'loss': 'margin',
   'lr': 0.01,
   'momentum': 0.005,
   'num_classes': 2,
   'num_epochs': 2000,
   'num_hidden_graph_layers': 100,
   'num_negative_samples': 50,
   'num_node_features': 50,
   'num_workers': 0,
   'optimiser': 'SGD',
   'test_p': 0.1,
   'valid_p': 0.2}

# COMMAND ----------

# MAGIC %md-sandbox
# MAGIC <div style="float:right">
# MAGIC   <img src="https://github.com/grandintegrator/gnn-db-blogpost/blob/main/media/logged_model.gif?raw=True" alt="graph-training" width="700px", />
# MAGIC </div>
# MAGIC
# MAGIC ### 4.3.3 Logging the GNN model into the model registry using mlflow
# MAGIC We will now take the parameters that were found with HyperOpt and create an mlflow run that will log a ```pyfunc``` flavour of our model along with a t-SNE plot of the learned embeddings. This can be viewed within the Experiments tab of Databricks. We notice that the embeddings form two large clusters. This shows **good learned embeddings!** since nodes that are connected should be clustered together and nodes that are not should be seperated. The decision boundary for the neural network will be simpler, can you see where the decision boundary would be? 
# MAGIC
# MAGIC This gives us confidence to move this model to production and classify the low confidence links based on the GNN.

# COMMAND ----------

# DBTITLE 1,Define a custom mlflow model for our GNN as a pyfunc flavour
class GNNWrapper(mlflow.pyfunc.PythonModel):
  def __init__(self, model, params):
    self.model = model
    self.params = params
    
  def test(self):
    print(self.params)
    
  def predict(self, context, model_input):
    # Create a graph structure with the model_input
    if isinstance(model_input, pd.DataFrame):
      source_company_id = np.array([x for x in model_input['src_id'].values])
      destination_company_id = np.array([x for x in model_input['dst_id'].values])
      g = dgl.graph((source_company_id, destination_company_id))
    else: 
      g = make_dgl_graph(model_input)
    
    # Assign node features to the new graph of size [1 x num_node_features]
    g.ndata['feature'] = torch.randn(g.num_nodes(),
                                     self.params['num_node_features'])
    with torch.no_grad():
      predictions = \
              self.model.get_embeddings(g=g,x=g.ndata['feature'],
                                        batch_size=self.params['batch_size'], 
                                        device='cpu',
                                        provide_prediction=True)
    return predictions.to('cpu').detach().numpy().squeeze()

# COMMAND ----------

# DBTITLE 1,We create a seperate mlflow run with the best parameters, log the model, and the t-SNE of the learned embeddings
with mlflow.start_run(run_name="Supply Chain GNN") as run:
  # Log the parameters of the model run
  mlflow.set_tag("link-prediction", "GraphSAGE")
  mlflow.log_params(best_parameters)
  run_id = run.info.run_id
  
  graph_model = Model(in_features=best_parameters['num_node_features'],
                  hidden_features=best_parameters['num_hidden_graph_layers'],
                  out_features=best_parameters['num_node_features'],
                  num_classes=best_parameters['num_classes'],
                  aggregator_type=best_parameters['aggregator_type'])

  data_loaders = get_edge_dataloaders(graph_partitions=graph_partitions,
                                      params=best_parameters)
  
  # ----------------------------------------------------------------------------
  # Collect results for training, validation, and testing
  # ----------------------------------------------------------------------------
  trainer = Trainer(params=best_parameters,
                    model=graph_model,
                    train_data_loader=data_loaders['training'],
                    training_graph=graph_partitions['training'])
  training_results = trainer.train_epochs()
  
  eval_aucs, eval_aps = evaluate(trained_model=training_results['model'],
                                 validation_data_loader=data_loaders['validation'])
  
  mlflow.log_metric("Validation AUC - mean", sum(eval_aucs)/len(eval_aucs))
  mlflow.log_metric("Validation AP - mean", sum(eval_aps)/len(eval_aps))
  
  test_aucs, test_aps = evaluate(trained_model=training_results['model'],
                                validation_data_loader=data_loaders['testing'])
   
  mlflow.log_metric("Testing AUC - mean", sum(test_aucs)/len(test_aucs))
  mlflow.log_metric("Testing AP - mean", sum(test_aps)/len(test_aps))

  # ----------------------------------------------------------------------------
  # Log the final trained model
  # ----------------------------------------------------------------------------
  gnn_model_pyfunc = GNNWrapper(training_results['model'], params=best_parameters)
  
  signature = infer_signature(silver_relation_table, 
                              gnn_model_pyfunc.predict(None,
                                                       model_input=silver_relation_table))
  mlflow.pyfunc.log_model(artifact_path="gnn_model", signature=signature,
                          python_model=gnn_model_pyfunc)
  
  # ----------------------------------------------------------------------------
  # We also log the t-SNE of the learned node embeddings, a key criteria!
  # ----------------------------------------------------------------------------
  with torch.no_grad():
    trained_model = training_results['model']
    training_graph = graph_partitions['training']
    training_graph_embeddings = (
        trained_model.get_embeddings(g=training_graph,
                                     x=training_graph.ndata['feature'],
                                     batch_size=params['batch_size'],
                                     device=params['device'])
    )
    t_sne_fig = plot_tsne_embeddings(graph_embeddings=training_graph_embeddings)
    mlflow.log_figure(t_sne_fig, 'visualisations/tsne_plot.html')

# COMMAND ----------

print(gnn_model_pyfunc.test())

# COMMAND ----------

# MAGIC %md-sandbox
# MAGIC ### 4.3.3 Finally, we register our model in the model registry
# MAGIC This model will then be used for inference to refine our silver table. See [next notebook!](https://e2-demo-field-eng.cloud.databricks.com/?o=1444828305810485#notebook/2045410163884197)

# COMMAND ----------

mlflow.register_model('runs:/' + run_id + '/gnn_model', model_name)

# COMMAND ----------


